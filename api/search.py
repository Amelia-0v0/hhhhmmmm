# /api/search.py (æœ€ç»ˆ Tavily API ç¨³å®šç‰ˆ)

import os
import httpx
import json
from flask import Flask, request, Response, stream_with_context
from flask_cors import CORS
# ğŸ‘‡ å¯¼å…¥ TavilyClient
from tavily import TavilyClient
from openai import OpenAI

app = Flask(__name__ )
CORS(app)

# --- ä»ç¯å¢ƒå˜é‡ä¸­è·å– Tavily API Key ---
# ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬ä¸æŠŠ Key ç¡¬ç¼–ç åœ¨ä»£ç é‡Œ
# ä½ éœ€è¦åœ¨è¿è¡Œåç«¯å‰ï¼Œåœ¨ç»ˆç«¯è®¾ç½®è¿™ä¸ªç¯å¢ƒå˜é‡
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")

def generate_event(event_type, data):
    """å°†æ•°æ®æ ¼å¼åŒ–ä¸ºæœåŠ¡å™¨å‘é€äº‹ä»¶ (SSE) å­—ç¬¦ä¸²ã€‚"""
    return f"event: {event_type}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"

@app.route('/api/search', methods=['POST'])
def search_handler():
    # --- è·å–è¯·æ±‚æ•°æ® (ä¸å˜) ---
    auth_header = request.headers.get('Authorization')
    openrouter_api_key = auth_header.split(' ')[1] if auth_header and auth_header.startswith('Bearer ') else None
    data = request.get_json()
    query = data.get('query')
    model = data.get('model')

    if not all([openrouter_api_key, query, model]):
        return Response(generate_event("error", {"message": "è¯·æ±‚å‚æ•°ä¸å®Œæ•´"}), status=400, mimetype='text/event-stream')
    
    # --- æ£€æŸ¥ Tavily Key æ˜¯å¦å·²è®¾ç½® ---
    if not TAVILY_API_KEY:
        # æä¾›æ¸…æ™°çš„é”™è¯¯æç¤ºï¼ŒæŒ‡å¯¼å¦‚ä½•ä¿®å¤
        error_msg = "æœåŠ¡å™¨ç¼ºå°‘ Tavily API Key é…ç½®ã€‚è¯·åœ¨å¯åŠ¨åç«¯æœåŠ¡çš„ç»ˆç«¯ä¸­ï¼Œä½¿ç”¨ 'set TAVILY_API_KEY=ä½ çš„Key' (Windows) æˆ– 'export TAVILY_API_KEY=ä½ çš„Key' (Mac/Linux) å‘½ä»¤è¿›è¡Œè®¾ç½®ã€‚"
        return Response(generate_event("error", {"message": error_msg}), status=500, mimetype='text/event-stream')

    def generate_responses():
        try:
            # åˆå§‹åŒ–ä¸¤ä¸ªå®¢æˆ·ç«¯
            openai_client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=openrouter_api_key, http_client=httpx.Client( ))
            tavily_client = TavilyClient(api_key=TAVILY_API_KEY)
            
            # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨ Tavily API è¿›è¡Œæœç´¢ ---
            yield generate_event("status", {"message": "æ­£åœ¨ä½¿ç”¨ Tavily AI è¿›è¡Œä¸“ä¸šæœç´¢..."})
            
            # è°ƒç”¨ Tavily çš„ search æ–¹æ³•ï¼Œå®ƒä¼šè¿”å›ä¸€ä¸ªç»“æ„åŒ–çš„ã€å¹²å‡€çš„ JSON
            # include_raw_content=False è¡¨ç¤ºæˆ‘ä»¬ä¸éœ€è¦åŸå§‹ç½‘é¡µHTMLï¼Œåªè¦æ€»ç»“å¥½çš„å†…å®¹
            search_result = tavily_client.search(query=query, search_depth="basic", max_results=5)
            
            # ä»è¿”å›ç»“æœä¸­æ„å»ºä¸Šä¸‹æ–‡
            search_context = "--- ä»¥ä¸‹æ˜¯ Tavily AI æœç´¢åˆ°çš„ç›¸å…³èƒŒæ™¯ä¿¡æ¯ ---\n"
            for result in search_result['results']:
                search_context += f"[æ¥æº: {result['url']}]\n{result['content']}\n\n"
            search_context += "--- èƒŒæ™¯ä¿¡æ¯ç»“æŸ ---\n\n"
            
            yield generate_event("search_results", {"context": search_context})

            # --- åç»­è°ƒç”¨ LLM çš„ä»£ç å®Œå…¨ä¸å˜ ---
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„å®æ—¶èƒŒæ™¯ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ä¼˜å…ˆã€æ·±å…¥åœ°åˆ©ç”¨è¿™äº›ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œåˆç†çš„æ€»ç»“ä¸æ¨ç†ã€‚å¦‚æœèƒŒæ™¯ä¿¡æ¯ä¸è¶³æˆ–æ²¡æœ‰æä¾›ï¼Œè¯·ç›´æ¥åˆ©ç”¨ä½ è‡ªèº«çš„çŸ¥è¯†è¿›è¡Œå›ç­”ã€‚"
            final_prompt = f"{search_context}è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œå›ç­”è¿™ä¸ªé—®é¢˜: \"{query}\""
            
            stream = openai_client.chat.completions.create(
                model=model,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": final_prompt}],
                stream=True,
            )

            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    yield generate_event("llm_chunk", {"content": content})
            
            yield generate_event("done", {"message": "Stream finished"})

        except Exception as e:
            print(f"An error occurred during streaming: {e}")
            import traceback
            traceback.print_exc()
            yield generate_event("error", {"message": f"æœåŠ¡å™¨å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}"})
    
    return Response(stream_with_context(generate_responses()), mimetype='text/event-stream')

# --- ç”¨äºæœ¬åœ°è¿è¡Œçš„å¯åŠ¨ä»£ç  (ä¸å˜) ---
if __name__ == '__main__':
    app.run(port=5001, debug=True)
